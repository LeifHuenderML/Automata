{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model\n",
    "import random\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GameOfLifeEnv(gym.Env):\n",
    "    def __init__(self, grid_size=1024):\n",
    "        super(GameOfLifeEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        #initialize the grid to be a boolean matrix of zeros of the grid size\n",
    "        #self.grid = torch.(self.grid_size, self.grid_size, dtype=torch.float32)\n",
    "        #for testing starts with a rand initialization\n",
    "        self.grid = torch.randint(low=0, high=2, size=(grid_size,grid_size), dtype=torch.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        #action is used to update the grid before the step\n",
    "        self.update_grid(action)\n",
    "        kernel = torch.tensor([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        result = F.conv2d(self.grid.unsqueeze(0).unsqueeze(0), kernel, padding=1)\n",
    "        output = ((result == 3) | ((result == 2) & (self.grid.unsqueeze(0).unsqueeze(0) == 1))).float()\n",
    "        next_state = output.squeeze()\n",
    "        reward = self.calc_reward(next_state)\n",
    "        self.grid = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = torch.zeros(self.grid_size, self.grid_size, dtype=torch.float32)\n",
    "        return self.grid\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plt.imshow(self.grid, cmap='gray_r')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    def update_grid(self, actions):\n",
    "        for i in range(0, actions.size(1), 2):\n",
    "            x = int(actions[0][i])\n",
    "            y = int(actions[0][i+1])\n",
    "            self.grid[x][y] = 1\n",
    "\n",
    "\n",
    "    def calc_reward(self, next_state):\n",
    "        state_count = self.count_living(self.grid)\n",
    "        next_state_count = self.count_living(next_state)\n",
    "        return next_state_count / state_count\n",
    "\n",
    "    def count_living(self, grid):\n",
    "        count = 0\n",
    "        for i in range(grid.size(0)):\n",
    "            for j in range(grid.size(1)):\n",
    "                if(grid[i][j] == 1):\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*64*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class AutomataSolver:\n",
    "    def __init__(self, n_episodes=1000, gamma=1.0, epsilon=1.0, epsilon_min=0.01, alpha=0.01, alpha_decay=0.01, batch_size=64, n_steps=1000):\n",
    "        self.n_episodes = n_episodes\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.env = GameOfLifeEnv()\n",
    "        self.model = DQN()\n",
    "        self.memory = []\n",
    "        self.memory_capacity = 10000\n",
    "        self.memory_position = 0\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.n_episodes):\n",
    "            state = self.env.reset().unsqueeze(0)\n",
    "            for step in range(self.n_steps):\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward = self.env.step(action)\n",
    "                next_state = next_state.unsqueeze(0) \n",
    "                self.remember(state, next_state, reward)\n",
    "                \n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    states, next_states, rewards = self.sample(self.batch_size)\n",
    "                    \n",
    "\n",
    "                    current_q_values = self.model(states).gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
    "                    max_next_q_values = self.model(next_states).detach().max(1)[0]\n",
    "                    expected_q_values = rewards + (self.gamma * max_next_q_values)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.criterion(current_q_values, expected_q_values)\n",
    "                    \n",
    "                    # Gradient descent\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.opt.step()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                # Epsilon decay\n",
    "                if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon -= (self.epsilon - self.epsilon_min) * self.alpha_decay\n",
    "\n",
    "                \n",
    "    def test(self):\n",
    "        for episode in range(self.n_episodes):\n",
    "            state = self.env.reset().unsqueeze(0)\n",
    "            for step in range(self.n_steps):\n",
    "                action = self.get_action(state)\n",
    "                print(f'actionshape: {action.shape}')\n",
    "                next_state, reward = self.env.step(action)\n",
    "                next_state = next_state.unsqueeze(0) \n",
    "                self.remember(state, next_state, reward)\n",
    "                \n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    states, next_states, rewards = self.sample(self.batch_size)\n",
    "                    \n",
    "\n",
    "                    current_q_values = self.model(states).gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
    "                    max_next_q_values = self.model(next_states).detach().max(1)[0]\n",
    "                    expected_q_values = rewards + (self.gamma * max_next_q_values)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.criterion(current_q_values, expected_q_values)\n",
    "                    \n",
    "                    # Gradient descent\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.opt.step()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                # Epsilon decay\n",
    "                if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon -= (self.epsilon - self.epsilon_min) * self.alpha_decay\n",
    "\n",
    "    def remember(self, state, next_state, reward):\n",
    "        if len(self.memory) < self.memory_capacity: \n",
    "            self.memory.append(None)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        self.memory[self.memory_position] = (state, next_state, reward_tensor)\n",
    "        self.memory_position = (self.memory_position + 1) % self.memory_capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state, next_state, reward = zip(*batch)\n",
    "        return torch.stack(state), torch.stack(next_state), torch.stack(reward) \n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0) \n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return q_values\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n",
      "actionshape: torch.Size([1, 200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "gather(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m automata \u001b[38;5;241m=\u001b[39m AutomataSolver()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mautomata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 149\u001b[0m, in \u001b[0;36mAutomataSolver.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m    146\u001b[0m     states, next_states, rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 149\u001b[0m     current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m     max_next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(next_states)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    151\u001b[0m     expected_q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m max_next_q_values)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: gather(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "automata = AutomataSolver()\n",
    "automata.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea run 1000 episodes of the game of life over 100 timesteps based on the original game rules \n",
    "# then run the model on the same game of life and see how well it does compared to the random initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
