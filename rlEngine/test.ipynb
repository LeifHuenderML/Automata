{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: DQN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=200, bias=True)\n",
      ")\n",
      "env: <GameOfLifeEnv instance>\n",
      "Number of parameters: 11194952\n",
      "Model size: 44.786002 MB\n",
      "Training Started...\n",
      "Episode: 0 of 200\n",
      "|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#Episode: 1 of 200\n",
      "|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#Episode: 2 of 200\n",
      "|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#Episode: 3 of 200\n",
      "|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#Episode: 4 of 200\n",
      "|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#|#"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "'''\n",
    "uses openais gym environment to create a game of life environment\n",
    "modified to use pytorch tensors and convolutions\n",
    "\n",
    "methods:\n",
    "    __init__(): initializes the environment with a grid size of 1024, and creates a grid of random 1s and 0s\n",
    "    step(action): updates the grid based on the action, calculates the reward, and checks if the game is done\n",
    "    check_done(next_state): checks if the game is done\n",
    "    reset(): resets the grid to a random state\n",
    "    render(state): renders the grid on a matplotlib plot\n",
    "    update_grid(actions): updates the grid based on the actions\n",
    "    calc_reward(next_state): calculates the reward based on the next state\n",
    "    count_living(grid): counts the number of living cells in the grid\n",
    "'''\n",
    "class GameOfLifeEnv(gym.Env):\n",
    "    def __init__(self, grid_size=1024):\n",
    "        super(GameOfLifeEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        #set the device to run the computations on the gpu if available\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        #initialize the grid with random 1s and 0s\n",
    "        self.grid = torch.randint(low=0, high=2, size=(grid_size,grid_size), dtype=torch.float32, device=self.device)\n",
    "        #initialize the kernel for the convolution\n",
    "        self.kernel = torch.tensor([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=torch.float32, device=self.device).view(1, 1, 3, 3)\n",
    "\n",
    "    def step(self, action):\n",
    "        #update the grid based on the action\n",
    "        self.update_grid(action)\n",
    "        #apply the convolution to the grid\n",
    "        result = F.conv2d(self.grid.unsqueeze(0).unsqueeze(0), self.kernel, padding=1)\n",
    "        #apply the rules of the game of life\n",
    "        output = ((result == 3) | ((result == 2) & (self.grid.unsqueeze(0).unsqueeze(0) == 1))).float()\n",
    "        #get the next state\n",
    "        next_state = output.squeeze()\n",
    "        reward = self.calc_reward(next_state)\n",
    "        self.grid = next_state\n",
    "        done = self.check_done(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "    #check if the game is done\n",
    "    def check_done(self, next_state):\n",
    "        if self.count_living(next_state) == self.grid_size * self.grid_size:\n",
    "            return True\n",
    "        else: return False\n",
    "    #reset the grid to a random state\n",
    "    def reset(self):\n",
    "        self.grid = torch.randint(low=0, high=2, size=(self.grid_size,self.grid_size), dtype=torch.float32, device=self.device)\n",
    "        return self.grid\n",
    "    #render the grid with matplotlib\n",
    "    def render(self, state):\n",
    "        grid = state.to('cpu')\n",
    "        plt.imshow(grid, cmap='gray_r')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    #update the grid based on the actions\n",
    "    def update_grid(self, actions):\n",
    "        #unpack the actions\n",
    "        for i in range(0, actions.size(1), 2):\n",
    "            x = int(actions[0][i])\n",
    "            y = int(actions[0][i+1])\n",
    "            #update the grid based on the x and y coordinates\n",
    "            self.grid[x][y] = 1\n",
    "        \n",
    "    #calculate the reward based on the next state and the current state of the grid\n",
    "    def calc_reward(self, next_state):\n",
    "        state_count = self.count_living(self.grid)\n",
    "        next_state_count = self.count_living(next_state)\n",
    "        return next_state_count / state_count\n",
    "    #count the number of living cells in the grid\n",
    "    def count_living(self, grid):\n",
    "        return torch.sum(grid == 1).item()\n",
    "\n",
    "'''\n",
    "DQN model for the game of life environment\n",
    "convolutional neural network with 5 convolutional layers and 4 fully connected layers\n",
    "methods:\n",
    "    __init__(): initializes the model with 5 convolutional layers and 4 fully connected layers\n",
    "    forward(x): forward pass of the model\n",
    "'''\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "         \n",
    "        self.fc1 = nn.Linear(4096, 2048)  \n",
    "        self.fc2 = nn.Linear(2048, 1024)  \n",
    "        self.fc3 = nn.Linear(1024, 256)\n",
    "        self.fc4 = nn.Linear(256, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "#used for pusing to the memory stack    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "#memory class for storing the transitions\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Trainer class for training the DQN model\n",
    "methods:\n",
    "    __init__(): initializes the trainer with the environment, model, device, episodes, iterations, batch size, memory size, gamma, learning rate, tau, optimizer, losses, and rewards\n",
    "    train(): trains the model over the specified number of episodes and iterations\n",
    "    optimize(): optimizes the model by calculating the loss and updating the weights\n",
    "    sanity_check(): prints the model, environment, number of parameters, and model size\n",
    "    get_action(state): gets the action based on the state\n",
    "    save_model(path): saves the model to the specified path\n",
    "    load_model(path): loads the model from the specified path\n",
    "    training_bar(iteration): prints a progress bar for the training\n",
    "    plot_metrics(): plots the losses per episode\n",
    "attributes:\n",
    "    env: the game of life environment\n",
    "    model: the DQN model\n",
    "    device: the device to run the computations on\n",
    "    episodes: the number of episodes to train the model\n",
    "    iterations: the number of iterations per episode\n",
    "    batch_size: the batch size for training\n",
    "    memory: the memory for storing the transitions\n",
    "    target_net: the target network for updating the weights\n",
    "    lr: the learning rate\n",
    "    tau: the tau value for updating the target network\n",
    "    optimizer: the optimizer for training the model\n",
    "    losses: the losses per episode\n",
    "    rewards: the rewards per episode'''\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, episodes=200, iterations=1000, batch_size=32, lr=1e-4, memory_size=1000, tau=0.005):\n",
    "        self.env = GameOfLifeEnv()\n",
    "        self.model = DQN()\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.episodes = episodes\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = Memory(memory_size)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.model.to(self.device)\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training Started...\")\n",
    "        for episode in range(self.episodes):\n",
    "            #reset the environment\n",
    "            state = self.env.reset()\n",
    "            iteration = 0\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            print(f'Episode: {episode} of {self.episodes}')\n",
    "            #loop through the iterations\n",
    "            for iteration in range(self.iterations):\n",
    "                #print the progress bar\n",
    "                self.training_bar(iteration)\n",
    "                #get the action based on the state\n",
    "                state = state.unsqueeze(0).unsqueeze(1)\n",
    "                action = self.get_action(state)\n",
    "                #take a step in the environment\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                cum_reward += reward\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "                #push the transition to the memory\n",
    "                self.memory.push(state,action,next_state,reward)\n",
    "                #update the state\n",
    "                state = next_state\n",
    "                #optimize the model\n",
    "                loss = self.optimize()\n",
    "                #append the loss\n",
    "                self.losses.append(loss)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                iteration += 1\n",
    "            self.rewards.append(cum_reward/iteration)\n",
    "            \n",
    "        print(\"Training Ended...\")\n",
    "        self.plot_metrics()\n",
    "        \n",
    "\n",
    "    def optimize(self):\n",
    "        #if the memory is less than the batch size dont optimize\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  \n",
    "        #sample the transitions\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        #concatenate the states and actions\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        #get the model output\n",
    "        model_output = self.model(state_batch)\n",
    "        #get the expected values\n",
    "        expected_values = torch.repeat_interleave(reward_batch.unsqueeze(1), 200, dim=1)  \n",
    "        criterion = nn.MSELoss() \n",
    "        #calculate the loss\n",
    "        loss = criterion(model_output,expected_values)\n",
    "        #zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #backpropagate the loss\n",
    "        loss.backward()\n",
    "        #clip the gradients\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "        #update the weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def sanity_check(self):\n",
    "            print(f\"model: {self.model}\")\n",
    "            print(f\"env: {self.env}\")\n",
    "            n_params = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f\"Number of parameters: {n_params}\")\n",
    "            torch.save(self.model.state_dict(), 'sanity_check_model.pth')\n",
    "            model_size_mb = os.path.getsize('sanity_check_model.pth') / 1e6\n",
    "            print(f\"Model size: {model_size_mb} MB\")\n",
    "    #get the action based on the state\n",
    "    def get_action(self, state):\n",
    "        #randomly select an action\n",
    "        if random.random() > 0.1:\n",
    "            return torch.randint(low=0, high=self.env.grid_size, size=(1, 200), dtype=torch.float32, device=self.env.device)   \n",
    "        else:\n",
    "        #get the model output\n",
    "            return self.model(state)\n",
    "        \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def training_bar(self, iteration):\n",
    "        if(iteration %10 == 0): print('|#', end='')\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.losses, label='Loss')\n",
    "        plt.title('Loss per Episode')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "automata = Trainer(episodes=200)\n",
    "automata.sanity_check()\n",
    "automata.train()\n",
    "automata.save_model('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea run 1000 episodes of the game of life over 100 timesteps based on the original game rules \n",
    "# then run the model on the same game of life and see how well it does compared to the random initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
